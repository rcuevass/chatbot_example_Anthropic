{
  "2405.07767v1": {
    "title": "Synthetic Test Collections for Retrieval Evaluation",
    "authors": [
      "Hossein A. Rahmani",
      "Nick Craswell",
      "Emine Yilmaz",
      "Bhaskar Mitra",
      "Daniel Campos"
    ],
    "summary": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2405.07767v1",
    "published": "2024-05-13"
  },
  "2406.03628v2": {
    "title": "Synthetic Oversampling: Theory and A Practical Approach Using LLMs to Address Data Imbalance",
    "authors": [
      "Ryumei Nakada",
      "Yichen Xu",
      "Lexin Li",
      "Linjun Zhang"
    ],
    "summary": "Imbalanced classification and spurious correlation are common challenges in\ndata science and machine learning. Both issues are linked to data imbalance,\nwith certain groups of data samples significantly underrepresented, which in\nturn would compromise the accuracy, robustness and generalizability of the\nlearned models. Recent advances have proposed leveraging the flexibility and\ngenerative capabilities of large language models (LLMs), typically built on\ntransformer architectures, to generate synthetic samples and to augment the\nobserved data. In the context of imbalanced data, LLMs are used to oversample\nunderrepresented groups and have shown promising improvements. However, there\nis a clear lack of theoretical understanding of such synthetic data approaches.\nIn this article, we develop novel theoretical foundations to systematically\nstudy the roles of synthetic samples in addressing imbalanced classification\nand spurious correlation. Specifically, we first explicitly quantify the\nbenefits of synthetic oversampling. Next, we analyze the scaling dynamics in\nsynthetic data augmentation, and derive the corresponding scaling law. Finally,\nwe demonstrate the capacity of transformer models to generate high-quality\nsynthetic samples. We further conduct extensive numerical experiments to\nvalidate the efficacy of the LLM-based synthetic oversampling and augmentation.",
    "pdf_url": "http://arxiv.org/pdf/2406.03628v2",
    "published": "2024-06-05"
  },
  "2502.14921v2": {
    "title": "The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
    "authors": [
      "Matthieu Meeus",
      "Lukas Wutschitz",
      "Santiago Zanella-BÃ©guelin",
      "Shruti Tople",
      "Reza Shokri"
    ],
    "summary": "How much information about training samples can be leaked through synthetic\ndata generated by Large Language Models (LLMs)? Overlooking the subtleties of\ninformation flow in synthetic data generation pipelines can lead to a false\nsense of privacy. In this paper, we assume an adversary has access to some\nsynthetic data generated by a LLM. We design membership inference attacks\n(MIAs) that target the training data used to fine-tune the LLM that is then\nused to synthesize data. The significant performance of our MIA shows that\nsynthetic data leak information about the training data. Further, we find that\ncanaries crafted for model-based MIAs are sub-optimal for privacy auditing when\nonly synthetic data is released. Such out-of-distribution canaries have limited\ninfluence on the model's output when prompted to generate useful,\nin-distribution synthetic data, which drastically reduces their effectiveness.\nTo tackle this problem, we leverage the mechanics of auto-regressive models to\ndesign canaries with an in-distribution prefix and a high-perplexity suffix\nthat leave detectable traces in synthetic data. This enhances the power of\ndata-based MIAs and provides a better assessment of the privacy risks of\nreleasing synthetic data generated by LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2502.14921v2",
    "published": "2025-02-19"
  },
  "2502.06555v1": {
    "title": "Is API Access to LLMs Useful for Generating Private Synthetic Tabular Data?",
    "authors": [
      "Marika Swanberg",
      "Ryan McKenna",
      "Edo Roth",
      "Albert Cheu",
      "Peter Kairouz"
    ],
    "summary": "Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application.",
    "pdf_url": "http://arxiv.org/pdf/2502.06555v1",
    "published": "2025-02-10"
  },
  "2504.12563v2": {
    "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
    "authors": [
      "Haris Riaz",
      "Sourav Bhabesh",
      "Vinayak Arannil",
      "Miguel Ballesteros",
      "Graham Horwood"
    ],
    "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
    "pdf_url": "http://arxiv.org/pdf/2504.12563v2",
    "published": "2025-04-17"
  }
}